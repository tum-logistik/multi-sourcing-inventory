{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d313941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from opt.mc_sim import *\n",
    "from common.variables import *\n",
    "from sim.sim_functions import *\n",
    "import torch\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, MultiDiscrete, Tuple, MultiBinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3063f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SourcingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37385bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGymEnv(Env):\n",
    "        \n",
    "    def __init__(self, sourcing_env):\n",
    "        self.SourcingEnv = sourcing_env\n",
    "        self.counter = 0\n",
    "        \n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = MultiDiscrete([2,INVEN_LIMIT])\n",
    "\n",
    "        # Inventory Observation State\n",
    "        self.observation_space = Box(-30,30,shape=(5,), dtype=int)\n",
    "        #Tuple(Box(-30,30,shape=(1,), dtype=int), Discrete(30), Discrete(30), MultiBinary(2))\n",
    "                                       \n",
    "    \n",
    "    def step(self, action):        \n",
    "        reward = self.reward_func(self.SourcingEnv.current_state, action)\n",
    "        next_state, event, i, event_probs, supplier_index = self.SourcingEnv.step(action)\n",
    "        self.counter += 1\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        if self.counter < PERIODS:\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "        \n",
    "        next_state_array = np.array(next_state.get_list_repr())\n",
    "        return next_state_array, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.SourcingEnv = SourcingEnv()\n",
    "        return np.array(self.SourcingEnv.current_state.get_list_repr())\n",
    "        \n",
    "    def reward_func(self, state, action):\n",
    "        reward_hb = H_COST * state.s if state.s >= 0 else B_PENALTY * state.s \n",
    "        reward = reward_hb + np.sum(np.multiply(action, PROCUREMENT_COST_VEC))\n",
    "        reward = float(reward)\n",
    "        return -reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1475e856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INVEN_LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "983d5100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Discrete(INVEN_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae236ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SourcingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f3f0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_gym_env = CustomGymEnv(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3305f174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_state = custom_gym_env.SourcingEnv.current_state\n",
    "m_state.get_list_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b555858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIODS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "560ccf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERIODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14fe29c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 3, 1, 1]), -60.0, False, {})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_gym_env.step(np.array([1,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1457550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985a9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(custom_gym_env) #warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45932b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a728ad57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Cost:-60.0 Observation (array([0, 1, 6, 1, 1]), -50.0, True, {})\n",
      "Episode:2 Cost:-65.0 Observation (array([1, 1, 8, 1, 1]), -65.0, True, {})\n",
      "Episode:3 Cost:-115.0 Observation (array([14,  2, 14,  1,  1]), -115.0, True, {})\n",
      "Episode:4 Cost:-175.0 Observation (array([26,  2, 26,  1,  1]), -175.0, True, {})\n",
      "Episode:5 Cost:-35.0 Observation (array([7, 0, 7, 1, 1]), -35.0, True, {})\n",
      "Episode:6 Cost:-100.0 Observation (array([20,  0, 20,  1,  1]), -100.0, True, {})\n",
      "Episode:7 Cost:-170.0 Observation (array([25,  2, 25,  1,  1]), -170.0, True, {})\n",
      "Episode:8 Cost:-65.0 Observation (array([0, 1, 8, 1, 1]), -55.0, True, {})\n",
      "Episode:9 Cost:-125.0 Observation (array([25,  0, 25,  1,  1]), -125.0, True, {})\n",
      "Episode:10 Cost:-95.0 Observation (array([ 0,  2, 20,  0,  1]), -95.0, True, {})\n",
      "Episode:11 Cost:-55.0 Observation (array([10,  0, 11,  1,  1]), -45.0, True, {})\n",
      "Episode:12 Cost:-105.0 Observation (array([21,  0, 21,  1,  1]), -105.0, True, {})\n",
      "Episode:13 Cost:-5.0 Observation (array([-1,  0,  2,  1,  1]), -5.0, True, {})\n",
      "Episode:14 Cost:-180.0 Observation (array([27,  2, 27,  1,  1]), -180.0, True, {})\n",
      "Episode:15 Cost:-130.0 Observation (array([25,  0, 26,  1,  1]), -120.0, True, {})\n"
     ]
    }
   ],
   "source": [
    "episodes = 15\n",
    "for episode in range(1, episodes+1):\n",
    "    state = custom_gym_env.reset()\n",
    "    done = False\n",
    "    cost = 0\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = custom_gym_env.action_space.sample()\n",
    "        n_state, reward, done, info = custom_gym_env.step(action)\n",
    "        cost+=reward\n",
    "        observation = custom_gym_env.step(action)\n",
    "    print('Episode:{} Cost:{} Observation {}' .format(episode, cost, observation))\n",
    "custom_gym_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39aea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e912f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c5a5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff5d8e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", custom_gym_env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7efe9308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | -87.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 1191     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -82.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 973         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029239912 |\n",
      "|    clip_fraction        | 0.474       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.08       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.72e+03    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.135      |\n",
      "|    value_loss           | 1.04e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -71.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 918         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056751374 |\n",
      "|    clip_fraction        | 0.541       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.98       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.67e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.142      |\n",
      "|    value_loss           | 7.19e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fa3fcc567c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6e395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df55fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c89134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70c383d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-5.0, 0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, custom_gym_env, n_eval_episodes=10, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa189573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee90371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f86914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ea574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
